{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548fd510",
   "metadata": {},
   "source": [
    "# Part 2: Data processing\n",
    "\n",
    "Within this part, we focus on text processing of the downloaded files (Part 1) found in your newly created folder **dataset**. Importantly, it is fully reproducible and where not, decisions taken into account have been fully disclosed for the sake of reproducibility.\n",
    "\n",
    "Starting point: newly created folder dataset (as result of running Notebook1_preprocessing)\n",
    "\n",
    "Folder dataset should now contain 7 automatically downloaded pdfs and 1 manually downloaded pdf with updated title (as per Notebook1_preprocessing).  \n",
    "\n",
    "These files should be present within the folder **dataset**:\n",
    "1. 1983__book_chapter__Ng_N_Q_M_Jacquot_A_Abifarin_K_Goli_A_Ghesquiere_and_K_Miezan_Rice_germplasm_collection_and_conservation_in_west_Africa_P.pdf\n",
    "2. 1984__journal_article__Hargrove_Thomas_R_Trip_Report_Africa_and_India_Trip_report_Africa_and_India_T_R_Hargrove_February_7_to_march_3_1984_14_p.pdf\n",
    "3. 1985__book_chapter__Alam_M_S_K_Alluri_T_M_Masajo_Kaung_Zan_and_V_T_John_Upland_rice_improvement_in_humid_and_subhumid_tropics_of_West_Africa.pdf\n",
    "4. 1985__journal_article__Kaung_Zan_V_T_John_and_M_S_Alam_Rice_production_in_Africa_an_overview_In_Rice_Improvement_in_Eastern_Central_and_Souther.pdf\n",
    "5. 1985__seminar__Alam_M_S_V_T_John_and_Kaung_Zan_Insect_pests_and_diseases_of_rice_in_Africa_In_Rice_Improvement_in_Eastern_Central_and_S.pdf\n",
    "6. 1986__Roger_PA__Recent_studies_on_free_living_blue_green_algae_and_azolla_at_the_International_Rice_Research_Institute.pdf\n",
    "7. 1986__seminar__Swaminathan_M_S_Can_Africa_feed_itself_an_application_of_lessons_learned_in_Asia_to_the_challenge_facing_Africa_Presente.pdf\n",
    "8. 1986__seminar__Swaminathan_M_S_Sustainable_nutrition_security_in_Africa_lessons_from_Asia_Presented_at_the_Twelfth_Ministerial_Session_.pdf\n",
    "\n",
    "\n",
    "## Notebook setup\n",
    "\n",
    "Prior to working with this setup, please create correct environment as per **part2_requirements.yml**. In case you need to prepare kernel for JupyterNotebook, use the following code:\n",
    "\n",
    "`python -Xfrozen_modules=off -m ipykernel install --user --name=part2_env --display-name=\"part2_env\"`\n",
    "\n",
    "\n",
    "\n",
    "## Loading modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4736221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def read_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def preprocess_and_export_text(filename, filepath):\n",
    "    \n",
    "    text = read_text(os.path.join(filepath, filename))\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text).strip()\n",
    "    filename = filename.removesuffix('.txt')\n",
    "    \n",
    "    out = str(filename)\n",
    "    out = out[0:150] + '_norm.txt'\n",
    "    \n",
    "    new_filepath = filepath + \"_norm\"\n",
    "    os.makedirs(new_filepath, exist_ok=True)\n",
    "    text_file = new_filepath / Path(out)\n",
    "    \n",
    "    with open(text_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "\n",
    "def wfa(file,filepath,tool):\n",
    "    \n",
    "    text = read_text(os.path.join(filepath, file))\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    word_freq = dict(zip(vectorizer.get_feature_names_out(), X.toarray()[0]))\n",
    "    df = pd.DataFrame(list(word_freq.items()), columns=['Word', 'Frequency'])\n",
    "    df['OCR'] = tool\n",
    "    df['paper']= file\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_and_save(df):\n",
    "    papers = df['paper'].drop_duplicates().reset_index(drop=True)\n",
    "    os.makedirs(os.path.join(os.getcwd(),\"papers\"), exist_ok=True)\n",
    "    os.chdir(os.path.join(os.getcwd(),\"papers\"))\n",
    "    for i in range(0, len(papers)):\n",
    "        pap = papers[i]\n",
    "        chunk = df[df['paper']==pap]\n",
    "        chunk.to_excel(f'{pap.removesuffix(\".txt\")}_real.xlsx', index=False)\n",
    "    os.chdir(\"..\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed23a5",
   "metadata": {},
   "source": [
    "## Extracting text\n",
    "\n",
    "For the sake of extracting text from pdfs, we sought to identify the best open-source tools for this task. Hence, the following 2 tools have been used:\n",
    "1. [tesseract](https://pypi.org/project/pytesseract/)\n",
    "2. [doctr](https://github.com/mindee/doctr)\n",
    "\n",
    "Due to hardware limitations and speed, both of these tools have been run on our local high performance computing (HPC) cluster based on SLURM scheduler. For each of the tools, you can find the:\n",
    "1. Submission script (.sh) - information on the requirements needed to run these tasks in a parallel (job array) manner (doctr.sh, tesseract.sh)\n",
    "2. Python script (.py) - code performing the task of text extraction (doctr.py, tesseract.py)\n",
    "3. Requirements file (.yml) - information needed for creating virtual environments (doctr_requirements.yml, tesseract_requirements.yml)\n",
    "\n",
    "As such, if one would want to reproduce these processes on their local hardware, please check the requirements files as your starting point for building an conda environment, followed by respective python scripts for each task. Caution - the requirements files are both based on Linux-based cluster usage, so one might wants to check which libraries are unnecesarry to work with a local computer (e.g. CUDA related packages).\n",
    "\n",
    "Furthermore, for the sake of this repository, we have included text files as on output from both of the tools, as starting point for the next steps.  \n",
    "\n",
    "\n",
    "## Loading data\n",
    "\n",
    "The data for this notebook is found in folders **doctr** and **tesseract**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1217702",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining dataset paths\n",
    "\n",
    "doctr_path = os.path.join(os.getcwd(),\"doctr\")\n",
    "tess_path = os.path.join(os.getcwd(),\"tesseract\")\n",
    "\n",
    "input_txts_doctr = listdir(doctr_path)\n",
    "input_txts_tess = listdir(tess_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1cbea",
   "metadata": {},
   "source": [
    "## Text normalization\n",
    "\n",
    "First step of text post-processing is to standardize line breaks like removing extra newlines or spaces (text normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9c7ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in input_txts_doctr:\n",
    "    preprocess_and_export_text(file, doctr_path)\n",
    "    preprocess_and_export_text(file, tess_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4407d",
   "metadata": {},
   "source": [
    "## Word frequency analysis (WEF)\n",
    "\n",
    "At this step we are trying to idnetify unusual words that may indicate OCR errors. Furthermore, in case of duplicates these words will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85fa717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([['test', 0, 'tool','paper']], columns=['Word', 'Frequency','OCR','paper'])\n",
    "input_txts = listdir((doctr_path + \"_norm\"))\n",
    "\n",
    "for file in input_txts:\n",
    "    df1 = wfa(file, (doctr_path + \"_norm\"), 'doctr')\n",
    "    df2 = wfa(file, (tess_path + \"_norm\"), 'tesseract')\n",
    "    df = pd.concat([df, df1, df2], ignore_index = True)    \n",
    "\n",
    "df = df.drop(0, axis=0).reset_index(drop=True)\n",
    "\n",
    "## splitting our dataset per pdf\n",
    "split_and_save(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3996ee",
   "metadata": {},
   "source": [
    "## Realness checks\n",
    "\n",
    "Next step of preprocessing checks for the contextual appropriateness of words, or if existing word is \"real\". To do that, we will be mapping our previously identifed words to 2 different corpuses and a database:\n",
    "1. words ([nltk](https://www.nltk.org/)) - containing approx 236k words from Project Gutenberg. Includes British and American spellings, as well as some archaic and rare words (literature texts).\n",
    "2. en_core_web_lg ([spacy](https://spacy.io/models/en)) - containing approx 685k vectors of words from Common Crawl and Wikipedia. Includes web-scale data.\n",
    "3. pubmed database ([pubmed](https://pubmed.ncbi.nlm.nih.gov/)) - containing approx 36M article records with adapted vocabulary for research (e.g. rice research).\n",
    "\n",
    "\n",
    "Since checking against pubmed database relies on API calls, this process has also been outsourced to our local HPC. Hence, the files are created to support this task: realness.sh, realness.py, realness_requirements.yml. Resulting files from these process can be found in folder **realness**. These have been use to estimate the quality of the processing and used to determine if further steps are needed for text cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Paper  Realness_tess  \\\n",
      "0  1983__seminar__Ng_N_Q_M_Jacquot_A_Abifarin_K_G...           0.88   \n",
      "1  1985__book_chapter__Alam_M_S_V_T_John_and_Kaun...           0.86   \n",
      "2  1985__book_chapter__Kaung_Zan_V_T_John_and_M_S...           0.92   \n",
      "3  1985__proceeding__Alam_M_S_K_Alluri_T_M_Masajo...           0.89   \n",
      "4  1986__seminar__Roger_PA__Recent_studies_on_fre...           0.82   \n",
      "5  1986__seminar__Swaminathan_M_S_Sustainable_nut...           0.94   \n",
      "\n",
      "   Realness_doctr  \n",
      "0            0.89  \n",
      "1            0.86  \n",
      "2            0.91  \n",
      "3            0.89  \n",
      "4            0.84  \n",
      "5            0.93  \n"
     ]
    }
   ],
   "source": [
    "## reading in files from realness dataset\n",
    "realness_path = os.path.join(os.getcwd(),\"realness\")\n",
    "\n",
    "input_real = listdir(realness_path)\n",
    "\n",
    "df_real = []\n",
    "for file in input_real:\n",
    "    score = pd.read_excel(os.path.join(realness_path, file))\n",
    "    realness_tess = score['realness_tess']\n",
    "    the_score_tess = realness_tess[1]\n",
    "    realness_doctr = score['realness_doctr']\n",
    "    the_score_doctr = realness_doctr[1]\n",
    "    the_paper = f\"{file}\"\n",
    "    df_real.append({'Paper':the_paper,'Realness_tesseract':the_score_tess, 'Realness_doctr':the_score_doctr})\n",
    "    \n",
    "df_real_table = pd.DataFrame(df_real)\n",
    "\n",
    "print(df_real_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d2e3a",
   "metadata": {},
   "source": [
    "With this process, we have wrapped up our text processing pipeline for the input to the model (Part 3).     \n",
    "\n",
    "## (Optional) Manual editing\n",
    "\n",
    "Will be directly discussed within the paper.\n",
    "\n",
    "## (Optional) Spelling correction\n",
    "\n",
    "Already at this level, the quality of text is high enough to serve as an input to the model, however it is work in progress to increase the text quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754422f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_building_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
